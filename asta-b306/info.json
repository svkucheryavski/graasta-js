{
   "id": "asta-b306",
   "title": "Cross-validation",
   "info": "Cross-validation, model performance and overfitting.",
   "video": "",
   "help": "<h2>Cross-validation</h2> <p>Cross-validation is a way to estimate the sampling error without taking new samples from the population. The idea is to split your original observations to several segments and then, for each segment, make a local model by taking the segment observations out of the dataset, and use the rest for training the model. After that, the local model is used to predict the response values of the excluded observations. The procedure is repeated for each segment, so at the end every observation will have a predicted response value (<em>y</em><sub>cv</sub>).</p> <p>Splitting the observations into segments can be done in several ways. The simplest is to consider every observation as individual segment. In this case the number of segments is equal to the number of observations and thus on every step one observation will be taking out, while the rest will be used for training the local model. This way is called <em>leave-one-out</em> or <em>full cross-validation</em>. Alternatively you can assign two or more observations to each segment. This can be done randomly (<em>random segmented cross-validation</em>) or systematically by taking every k-th observation (<em>venetian blinds</em>).</p> <p>This app shows how all three methods work for a simple dataset with 12 observations. If random or systematic split is selected, number of segments will be equal to 4. Cross-validation can help to detect overfitting, you can also test this in the app — try to use overfitted model, e.g. cubic polynomial and will see that despite the calibration error is getting smaller, the cross-validation error will be larger for overcomplicated models.</p>"
}
