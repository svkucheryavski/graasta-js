{
   "id": "asta-b307",
   "title": "Jackknifing",
   "info": "Jackknife resampling for regression coefficients.",
   "video": "",
   "help": "<h2>Jackknife resampling for regression coefficients</h2><p>Jackknife resampling is a way to make an inference (e.g. compute confidence intervals of find a p-value for H0: Î² = 0) for regression coefficients based on full cross-validation. The idea is to estimate a variance of regression coefficients computed for each local model (every time we take one sample out, we compute a new model, which is called <em>local</em>) and then compute the standard error for the coefficient based on this variance.</p> <p> In this app population consists of 500 measurements of <em>x</em> and <em>y</em>, where <em>y</em> linearly depends on <em>x</em>. However, if you take a small sample (in this app the sample size is fixed to <em>n</em> = 12), it can have a random non-linear effect in the <em>y</em>(<em>x</em>) relationship, so when this sample is fitted by a polynomial model, the regression coefficients for quadratic or cubic terms will not be zero. But if you use Jackknife, then most of the time it will be able to detect that the estimated coefficients are in fact not significant, so statistically they are not distinguishable from zero.</p> <p> In order to investigate this, set polynomial to <em>cubic</em> and take a sample where the last two regression coefficients will be relatively large, so you can see them as small blue bars on the coefficients' plot. Then run the cross-validation procedure and see how all coefficients vary for the local models. At the end you will see errorbars which correspond to 95% confidence intervals and for the non-linear terms, most of the time, the interval will cross zero.</p>"
}
